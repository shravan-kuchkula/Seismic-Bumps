---
title: "seismic.Rmd"
author: "Shravan Kuchkula, Dave Dyer, Tommy Pompo"
date: "12/03/2017"
output:
  github_document:
    toc: yes
  html_document:
    keep_md: yes
    theme: cosmo
    toc: yes
  pdf_document:
    fig_caption: yes
    highlight: zenburn
editor_options: 
  chunk_output_type: console
---

## Introduction

The dangers associated with coal mining are myriad; black lung, flammable gas pockets, rockbursts, and tunnel collapses are all very real dangers that mining companies must consider when attempting to provide safe working conditions for miners.  One class of mining hazard, commonly called 'seismic hazards', are notoriously difficult to protect against and even more difficult to predict with certainty.  Therefore, predicting these hazards has become a well-known problem for machine learning and predictive analytics.  The UCI Machine Learning Repository (https://archive.ics.uci.edu) provides a 'seismic bumps' data set that contains many records of combined categorical and numeric variables that could be used to predict seismic hazards.  This 'seismic bumps' data set can be found at https://archive.ics.uci.edu/ml/datasets/seismic-bumps. 

Our analysis attempts to use logistic regression techniques to predict whether a seismic 'bump' is predictive of a notable seismic hazard.  We attempt to characterize our prediction accuracy and compare the results against the state of the art results from other statistical and machine learning techniques, that are included within the data set.

## Getting the data

```{r message=FALSE, warning=FALSE}
source("libraries.R")
#url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/00266/seismic-bumps.arff"
#download.file(url, "seismic-bumps.arff")
seismicData <- import("seismic-bumps.arff")
```

## Data set description

The data were taken from from instruments in the Zabrze-Bielszowice coal mine, in Poland.  There are 2,584 records, with only 170 class = 1 variables, so the data are significantly skewed towards non-hazardous training data.  Field descriptions are below, but essentially energy readings and bump counts during one work shift are used to predict a 'hazardous' bump during the next shift.  From the data description, a 'hazardous bump' is a seismic event with > 10,000 Joules, and a 'shift' is a period of 8 hours. For the sake of reference, a practical example of 10,000 Joules would be the the approximate energy required to lift 10,000 tomatoes 1m above the ground.   A class = 1 variable result signifies that a harzardous bump did, indeed, occur in the following shift to the measured data.  Here is an example of the fields in the data set.  

```{r message=FALSE, echo = FALSE, warning=FALSE}
glimpse(seismicData)
nonhazardous <- seismicData %>%
    filter(class == "0")
nonhaz_nbumps_count <- nonhazardous %>%
    summarize(sum_nbumps = sum(nbumps),
              sum_nbumps2 = sum(nbumps2),
              sum_nbumps3 = sum(nbumps3),
              sum_nbumps4 = sum(nbumps4),
              sum_nbumps5 = sum(nbumps5),
              sum_nbumps6 = sum(nbumps6),
              sum_nbumps7 = sum(nbumps7),
              sum_nbumps89 = sum(nbumps89)
              )
hazardous <- seismicData %>%
    filter(class == "1") 
haz_nbumps_count <- hazardous %>%
    summarize(sum_nbumps = sum(nbumps),
              sum_nbumps2 = sum(nbumps2),
              sum_nbumps3 = sum(nbumps3),
              sum_nbumps4 = sum(nbumps4),
              sum_nbumps5 = sum(nbumps5),
              sum_nbumps6 = sum(nbumps6),
              sum_nbumps7 = sum(nbumps7),
              sum_nbumps89 = sum(nbumps89)
              )
nonhaz_bumps <- data.frame(col1 = c(1858, 849, 847, 150, 11, 0, 0, 0))  # note, this is ugly and static.
haz_bumps <- data.frame(col1 = c(363, 168, 168, 25, 1, 0, 0, 0))  # this too.
#row.names(bumps) <- names(haz_nbumps_count)
phaz <- ggplot(haz_bumps, aes(x=row.names(haz_bumps), y=col1)) + 
    geom_col(fill="goldenrod3", color="goldenrod4") +
    theme(axis.text.x = element_text(angle=45, hjust=1)) + 
    ylab("Count") + 
    xlab("Bump Variables (Summed)") + 
    ggtitle("Hazardous Bumps Breakdown")


pnonhaz <- ggplot(nonhaz_bumps, aes(x=row.names(nonhaz_bumps), y=col1)) + 
    geom_col(fill="coral3", color="coral4") +
    theme(axis.text.x = element_text(angle=45, hjust=1)) + 
    ylab("Count") + 
    xlab("Bump Variables (Summed)") + 
    ggtitle("NonHazardous Bumps Breakdown")
library(gridExtra)
grid.arrange(phaz, pnonhaz, ncol=2)
```

From the UCI Machine Learning Repository, these are the field descriptions:

  * **seismic**: result of shift seismic hazard assessment in the mine working obtained by the seismic method (a - lack of hazard, b - low hazard, c - high hazard, d - danger state); 
  * **seismoacoustic**: result of shift seismic hazard assessment in the mine working obtained by the seismoacoustic method; 
  * **shift**: information about type of a shift (W - coal-getting, N -preparation shift); 
  * **genergy**: seismic energy recorded within previous shift by the most active geophone (GMax) out of geophones monitoring the longwall; 
  * **gpuls**: a number of pulses recorded within previous shift by GMax; 
  * **gdenergy**: a deviation of energy recorded within previous shift by GMax from average energy recorded during eight previous shifts; 
  * **gdpuls**: a deviation of a number of pulses recorded within previous shift by GMax from average number of pulses recorded during eight previous shifts; 
  * **ghazard**: result of shift seismic hazard assessment in the mine working obtained by the seismoacoustic method based on registration coming form GMax only; 
  * **nbumps**: the number of seismic bumps recorded within previous shift; 
  * **nbumps2**: the number of seismic bumps (in energy range [10^2,10^3)) registered within previous shift; 
  * **nbumps3**: the number of seismic bumps (in energy range [10^3,10^4)) registered within previous shift; 
  * **nbumps4**: the number of seismic bumps (in energy range [10^4,10^5)) registered within previous shift; 
  * **nbumps5**: the number of seismic bumps (in energy range [10^5,10^6)) registered within the last shift; 
  * **nbumps6**: the number of seismic bumps (in energy range [10^6,10^7)) registered within previous shift; 
  * **nbumps7**: the number of seismic bumps (in energy range [10^7,10^8)) registered within previous shift; 
  * **nbumps89**: the number of seismic bumps (in energy range [10^8,10^10)) registered within previous shift;
  * **energy**: total energy of seismic bumps registered within previous shift; 
  * **maxenergy**: the maximum energy of the seismic bumps registered within previous shift; 
  * **class**: the decision attribute - '1' means that high energy seismic bump occurred in the next shift ('hazardous state'), '0' means that no high energy seismic bumps occurred in the next shift ('non-hazardous state').

### Variable Types and Cardinality

There are 18 input variables and one binary output variable ("class").  The data are mostly numeric with 4 categorical input variables.  However, some of the numeric values only contain a handful of discrete values which can be viewed as coded categorical variables.  In particular, maxenergy and the 'nbumps(n)' variables can be treated as categorical.  So, in short, we see the following breakdown in variable types:

```{r include=FALSE}
categorical <- c("seismic", "seismoacoustic", "shift", "ghazard", "nbumps", "nbumps2", "nbumps3", "nbumps4", "nbumps5", "nbumps6", "nbumps7", "nbumps89", "class")
continuous <- c("genergy", "gdpuls", "energy", "maxenergy")
```

The categorical variables are `r categorical` and the continuous variables are `r continuous`.  The output variable is 'class'.

```{r include=FALSE}
source("libraries.R")
seismicData <- import("seismic-bumps.arff")
seismicSummary <- seismicData %>%
    summarise_all(funs("Total" = n(),
                  "Nulls" = sum(is.na(.)),
                  "Filled" = sum(!is.na(.)),
                  "Cardinality" = length(unique(.)))) %>%
    melt() %>%
    separate(variable, into = c('variable', 'measure'), sep="_") %>%
    spread(measure, value)  %>%
    mutate(Complete = Filled/Total,
           Uniqueness = Cardinality/Total,
           Distinctness = Cardinality/Filled)
```

A table outlining the variables and some of their attributes is below:

```{r echo=FALSE, comment = NA}
print(seismicSummary[,c(1,2,4,5,7,8)], row.names=FALSE)
```



## Exploratory Data Analysis

It is important to understand how many observations are "hazardous state (class = 1)" and "non-hazardous state (class = 0)"

```{r}
table(seismicData$class)
```

As mentioned above, the data set output variable is highly skewed, and contains many more non-hazardous classes than it does hazardous classes.  

### How are nbumps distributed ?

Distribution of all nbumps: Use `cowplot` to display all nbumps in a grid.
```{r echo=FALSE, message=FALSE}
library(cowplot)

p1 <- seismicData %>%
  ggplot(aes(x = nbumps)) +
  geom_histogram() +
  theme(axis.text.y = element_text(size = 6), axis.text.x = element_text(size = 6),
        axis.title.y = element_text(size = 10), axis.title.x = element_text(size = 10),
        axis.ticks = element_blank())

p2 <- seismicData %>%
  ggplot(aes(x = nbumps2)) +
  geom_histogram() +
  theme(axis.text.y = element_text(size = 6), axis.text.x = element_text(size = 6),
        axis.title.y = element_text(size = 10), axis.title.x = element_text(size = 10),
        axis.ticks = element_blank())

p3 <- seismicData %>%
  ggplot(aes(x = nbumps3)) +
  geom_histogram() +
  theme(axis.text.y = element_text(size = 6), axis.text.x = element_text(size = 6),
        axis.title.y = element_text(size = 10), axis.title.x = element_text(size = 10),
        axis.ticks = element_blank())

p4 <- seismicData %>%
  ggplot(aes(x = nbumps4)) +
  geom_histogram() +
  theme(axis.text.y = element_text(size = 6), axis.text.x = element_text(size = 6),
        axis.title.y = element_text(size = 10), axis.title.x = element_text(size = 10),
        axis.ticks = element_blank())

p5 <- seismicData %>%
  ggplot(aes(x = nbumps5)) +
  geom_histogram() +
  theme(axis.text.y = element_text(size = 6), axis.text.x = element_text(size = 6),
        axis.title.y = element_text(size = 10), axis.title.x = element_text(size = 10),
        axis.ticks = element_blank())

p6 <- seismicData %>%
  ggplot(aes(x = nbumps6)) +
  geom_histogram() +
  theme(axis.text.y = element_text(size = 6), axis.text.x = element_text(size = 6),
        axis.title.y = element_text(size = 10), axis.title.x = element_text(size = 10),
        axis.ticks = element_blank())

p7 <- seismicData %>%
  ggplot(aes(x = nbumps7)) +
  geom_histogram() + 
  theme(axis.text.y = element_text(size = 6), axis.text.x = element_text(size = 6),
        axis.title.y = element_text(size = 10), axis.title.x = element_text(size = 10),
        axis.ticks = element_blank())

p89 <- seismicData %>%
  ggplot(aes(x = nbumps89)) +
  geom_histogram() +
  theme(axis.text.y = element_text(size = 6), axis.text.x = element_text(size = 6),
        axis.title.y = element_text(size = 10), axis.title.x = element_text(size = 10),
        axis.ticks = element_blank())

plot_grid(p1, p2, p3, p4, p5, p6, p7, p89, ncol = 2)

```


### Logistic Regression Assumptions


#### Linearity

#### Independence of Errors

#### Multi-collinearity

Collect all the numeric variables and check for multi-collinearity:

```{r}
seismicDataNumeric <- seismicData %>%
  select(genergy, gpuls, gdenergy, gdpuls, energy, maxenergy)
```

```{r}
# Create the correlation matrix
M <- round(cor(seismicDataNumeric), 2)

# Create corrplot
corrplot(M, method="pie", type = "lower")
```

## Problem with unbalanced class variable

To illustrate the problem with having an unbalanced class variable we compare accuracy of the logistic regression full model with the null model. 

### Full Model

We first built a logistic regression model taking all the observations and variables into account. The summary of this full model is shown in the figure.

```{r}
# Reimport the dataset and start fresh
sb <- import("seismic-bumps.arff")

# Build a full model
fullModel <- glm(class ~ . , data = sb, family = "binomial")

# Summary full model
summary(fullModel)

```

Next we predict using this full model and calculate the probability of getting a hazardous bump. To convert these probabilities to classes, we define a threshold. A good value for threshold is the mean of the original response variable. Probabilities greater than this threshold are categorized into hazardous bump class and probabilities lesser than the threshold are categorized into non-hazardous bump class. Model accuracy is then calculated by comparing with the actual class variable. 

```{r}
sb$prob <- predict(fullModel, type = "response")

# Find the actual probability
actualProbability <- mean(as.numeric(as.character(sb$class)))

# Use the actual probability to as threshold to generate predictions
sb$pred <- ifelse(sb$prob > actualProbability, 1, 0)

# Now calculate model accuracy
fullModelAccuracy <- mean(sb$pred == sb$class)

# Print it
fullModelAccuracy
```

Our calculation show that the logistic regression model with all the observations and variables made a correct prediction `r round(fullModelAccuracy, 2)`%  of the time. 

### Null Model

What if our model always predicted class == 0, then what is the accuracy ?

```{r}
sb$predNull <- 0

nullModelAccuracy <- mean(sb$predNull == sb$class)

nullModelAccuracy

```

The null model accuracy is 93.42%. With our fullModel accuracy of `r round(fullModelAccuracy, 2)`% the model is actually performing worse than if it were to predict class 0 for every record.

This illustrates that "rare events" create challenges for classification models. When one outcome is very rare predicting the opposite can result in very high accuracy. We have demonstrated that accuracy is a very misleading measure of model performance on imbalanced datasets. Graphing the model's performance better illustrates the tradeoff between a model that is overly agressive and one that is overly passive. In the next section we will create a ROC curve and compute the area under the curve (AUC) to evaluate the logistic regression model.

## Balancing the class variable
When balancing the class variable, a key question to ask ourselves is by how much should we shrink the number of observations for the class with most values. To aid in answering this question, we can use cross validation to figure out the optimal value to choose. We developed a function in R called `cVLogisticRegression(n, k, size) `- where `n` is the number of observations of the majority class, `k` is the number of folds and `size`  is the number of observations per fold to include in test and train data. The function returns a vector of “Area Under the Curve” AUC values.  A histogram of these AUC values for n = 170, n = 500, n = 1000 is shown in the figure.



```{r warning = FALSE}

cVLogisticRegression <- function(n, k, size){
  
  # Reimport the dataset and start fresh
  sb <- import("seismic-bumps.arff")

  # Extract non-hazardous observations
  nonh <- sb %>%
    filter(class == 0)

  # Randomly choose n observations
  nonh <- nonh[sample(1:nrow(nonh), n), ]

  # Extract hazardous observations
  h <- sb %>%
    filter(class == 1)

  # Combine the nonh and h dataframes
  balancedSB <- rbind(nonh, h)

  # Shuffle the dataframe
  balancedSB <- balancedSB[sample(1:nrow(balancedSB)),]

  # Extract the class variable into a vector call it "dat.train.y"
  dat.train.y <- balancedSB$class

  # Create a model matrix using the entire balancedSB. 
  # Earlier we split this into training/test data. 
  # Now we are going do that within the loop. 

  dat.train.x <- model.matrix(as.formula("class ~ ."), data = balancedSB)

  # Define the number of cross-validation loops
  nloops <- k

  # Number of observations in the training (i.e full) dataset
  ntrains <- nrow(dat.train.x)

  # Create an empty vector to hold all the AUC result.
  # Later we will display each run's AUC in a histogram
  cv.aucs <- c()

  for (i in 1:nloops){
  
   # Get the indexes using sample
   index<-sample(1:ntrains,size)
 
   # randomly draw observations from front and call it training set
   cvtrain.x<-as.matrix(dat.train.x[index,])
 
   # randomly draw observations from back and call it test set
   cvtest.x<-as.matrix(dat.train.x[-index,])
 
   # Get the corresponding class variable
   cvtrain.y<-dat.train.y[index]
   cvtest.y<-dat.train.y[-index]
 
   ## Model fitting:
   # Call cv.glmnet to fit the model using training set
   cvfit <- cv.glmnet(cvtrain.x, cvtrain.y, family = "binomial", type.measure = "class")
 
   ## Prediction:
   # Predict using test set using the above model. Use type = "response" to get prediction probabilities.
   fit.pred <- predict(cvfit, newx = cvtest.x, type = "response")
   
   # Prediction function takes prediction probabilities as first arg and class to compare as its second arg
   pred <- prediction(fit.pred[,1], cvtest.y)
 
   ## Prediction Performance:
   # Check prediction performance
   roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
   auc.train <- performance(pred, measure = "auc")
   auc.train <- auc.train@y.values
 
   # Store the auc value for this run into the vector
   cv.aucs[i]<-auc.train[[1]]

  }
  
  return(cv.aucs)

}
```

For n = 170
```{r warning=FALSE}
# n - number of class 0 obs
# k - number of folds to use in cv 
# size - size of each fold
aucVector <- cVLogisticRegression(170, 50, 60)
hist(aucVector, main = paste("Histogram of AUC for n = ", 170))
text(x = 0.75, y = 25, paste("Mean AUC = ", round(mean(aucVector), 2)), col = "blue")
```

For n = 500
```{r warning=FALSE}
# n - number of class 0 obs
# k - number of folds to use in cv 
# size - size of each fold
aucVector <- cVLogisticRegression(500, 50, 60)
hist(aucVector, main = paste("Histogram of AUC for n = ", 500))
text(x = 0.75, y = 25, paste("Mean AUC = ", round(mean(aucVector), 2)), col = "blue")
```

For n = 1000
```{r warning=FALSE}
# n - number of class 0 obs
# k - number of folds to use in cv 
# size - size of each fold
aucVector <- cVLogisticRegression(1000, 50, 60)
hist(aucVector, main = paste("Histogram of AUC for n = ", 1000))
text(x = 0.65, y = 22, paste("Mean AUC = ", round(mean(aucVector), 2)), col = "blue")
```

As we can see, as the n value increases (i.e imbalance increases) our model performance is decreasing. This suggests that we should carefully choose the n value while building our model.

The effect of balancing the class variable can be observed in the below table:

200 -> AUC was close to 80%
500 -> AUC was close to 75%
1000 -> AUC was close to 50%

This shows that logistic regression model's performance is largely dependent on having a balanced class variable.

## Logistic Regression using with balanced dataset with train/test split

We will first balance the class variable so that there are approximately equal number of hazardous vs non-hazardous cases.

```{r}
# Reimport the dataset and start fresh
sb <- import("seismic-bumps.arff")

# Extract non-hazardous observations
nonh <- sb %>%
  filter(class == 0)

# Randomly choose 200 observations
nonh <- nonh[sample(1:nrow(nonh), 200), ]

# Extract hazardous observations
h <- sb %>%
  filter(class == 1)

# Combine the nonh and h dataframes
balancedSB <- rbind(nonh, h)

# Shuffle the dataframe
balancedSB <- balancedSB[sample(1:nrow(balancedSB)),]

```

Next, we will split the dataset into training (75%) and test (25%) datasets.

```{r}
# Calculate N
N <- nrow(balancedSB)

# Create a random number vector
rvec <- runif(N)

# Select rows from the dataframe
balancedSB.train <- balancedSB[rvec < 0.75,]
balancedSB.test <- balancedSB[rvec >= 0.75,]

# Select rows for the class variable
train_class <- balancedSB.train$class
test_class <- balancedSB.test$class

```

Next, we build the model. 

Our goal is to select a model with the `smallest number of coefficients that also gives a good accuracy`. 

```{r}
#Build the formula for model.matrix
formula <- as.formula("class ~ .")

#Build the model matrix object from the training data
balancedSBmatrix <- model.matrix(formula, data = balancedSB.train)

# Pass the matrix and class vector
cvfit <- cv.glmnet(balancedSBmatrix, train_class, family = "binomial", type.measure = "class")

# plot the model
plot(cvfit)
```

We are essentially looking for the lamba value which yeilds the lowest miss-classification error. That lamba value will be the number of coefficients that are contributing the most.

Check the coefficients

```{r}
coef(cvfit, s = "lambda.min")
```

**Prediction**:

We will use the above model to predict on the test dataset. 

```{r}
# Since predict function expects a matrix to be passed to newx
# Note the use of balancedSB.test here instead of train
balancedSBmatrix <- model.matrix(formula, data = balancedSB.test)

# predict on the test data, use type = "response" to get prediction probabilities
pred <- predict(cvfit, newx = balancedSBmatrix, type = "response")

# create a prediction object
predObj <- prediction(pred, test_class)

```

**Prediction Performance**:
```{r}
# Measure the performance using true positive rate vs false positive rate
myroc.perf <- performance(predObj, measure = "tpr", x.measure = "fpr")

# Measure the performance using AUC
auc.test <- performance(predObj, measure = "auc")

# Get the AUC value to display on the ROC plot
auc.value <- auc.test@y.values

# Plot the ROC with AUC value
plot(myroc.perf)
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.value[[1]],3), sep = ""))

```



## Evaluating model performance using Cross Validation

Using cross validation we can assess how well our model building process works. The idea is that we can know how well our model will perform on new data not yet collected. We will use AUC as the performance metric.

```{r}
# Reimport the dataset and start fresh
sb <- import("seismic-bumps.arff")

# Extract non-hazardous observations
nonh <- sb %>%
  filter(class == 0)

# Randomly choose 200 observations
nonh <- nonh[sample(1:nrow(nonh), 200), ]

# Extract hazardous observations
h <- sb %>%
  filter(class == 1)

# Combine the nonh and h dataframes
balancedSB <- rbind(nonh, h)

# Shuffle the dataframe
balancedSB <- balancedSB[sample(1:nrow(balancedSB)),]

# Extract the class variable into a vector call it "dat.train.y"
dat.train.y <- balancedSB$class

# Create a model matrix using the entire balancedSB. 
# Earlier we split this into training/test data. 
# Now we are going do that within the loop. 

dat.train.x <- model.matrix(as.formula("class ~ ."), data = balancedSB)
```

Now we are ready for the cross-validation loop

```{r}
# Define the number of cross-validation loops
nloops <- 50

# Number of observations in the training (i.e full) dataset
ntrains <- nrow(dat.train.x)

# Create an empty vector to hold all the AUC result.
# Later we will display each run's AUC in a histogram
cv.aucs <- c()
```

Run the cross validation

```{r warning= FALSE}
for (i in 1:nloops){
  
 # Get the indexes using sample
 index<-sample(1:ntrains,60)
 
 # randomly draw 60 observations from front and call it training set
 cvtrain.x<-as.matrix(dat.train.x[index,])
 
 # randomly draw 60 observations from back and call it test set
 cvtest.x<-as.matrix(dat.train.x[-index,])
 
 # Get the corresponding class variable
 cvtrain.y<-dat.train.y[index]
 cvtest.y<-dat.train.y[-index]
 
 ## Model fitting:
 # Call cv.glmnet to fit the model using training set
 cvfit <- cv.glmnet(cvtrain.x, cvtrain.y, family = "binomial", type.measure = "class")
 
 ## Prediction:
 # Predict using test set using the above model. Use type = "response" to get prediction probabilities.
 fit.pred <- predict(cvfit, newx = cvtest.x, type = "response")
 # Prediction function takes prediction probabilities as first arg and class to compare as its second arg
 pred <- prediction(fit.pred[,1], cvtest.y)
 
 ## Prediction Performance:
 # Check prediction performance
 roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
 auc.train <- performance(pred, measure = "auc")
 auc.train <- auc.train@y.values
 
 # Store the auc value for this run into the vector
 cv.aucs[i]<-auc.train[[1]]

}
 
```


Draw a historgram of cv.aucs
```{r}
hist(cv.aucs)
text(x = 0.75, y = 22, paste("Mean AUC = ", round(mean(cv.aucs), 2)), col = "blue")
```

This indicates that a majority of time our model prediction performance lies between 70 to 75% 



## Comparing the performance of classification techniques. 

Within the ARFF file is a large table of various classification techniques and their results.  


Classification results using stratified 10-fold cross-validation repeated 10 times

 | Algorithm                   | Acc.      | BAcc.     | Acc.0 spec     | Acc.1  sense     | Size  |
 ------------------------------|-----------|-----------|-------------|-------------|-----|
 | q-ModLEM(entropy-RSS)   (1) | 80.2(5.1) | 69.1(6.2) | 81.90       | 56.35       | 27.5  |
 | q-ModLEM(entropy-Corr.) (1) | 82.9(4.5) | 67.9(7.2) | 85.15       | 50.65       | 45.5  |
 | MODLEM (2)                  | 92.5(0.8) | 52.6(2.8) | 98.58          | 6.65     | 145.5 |
 | MLRules(-M 30)  (3)         | 93.2(0.3) | 50.5(1.3) | 99.69       | 1.29        | 30    |
 | MLRules(-M 100) (3)         | 92.9(0.6) | 52.0(2.2) | 99.10       | 4.88        | 100   |
 | MLRules(-M 500) (3)         | 92.3(0.6) | 52.9(2.8) | 98.27       | 7.59        | 500   |
 | BRACID (4)                  | 87.5(0.4) | 62.0(2.6) | 91.38       | 32.71       | -     |
 | Jrip (Weka)                 | 93.0(0.6) | 51.4(2.4) | 99.35       | 3.47        | 1.8   |
 | PART (Weka)                 | 92.1(0.8) | 52.7(3.5) | 98.09       | 7.35        | 34    |
 | J48 (Weka)                  | 93.1(0.8) | 50.2(0.9) | 99.64       | 0.82        | 5.6   |
 | SimpleCart (Weka)           | 93.4(0.0) | 50.0(0.0) | 100         | 0.00        | 1.0   |
 | NaiveBayes (Weka)           | 86.7(2.0) | 64.7(5.8) | 90.08       | 39.41       | -     |
 | IB1 (Weka)                  | 89.4(1.6) | 55.3(4.8) | 94.54       | 16.06       | -     |
 | RandomForest(-I 100) (Weka) | 93.1(0.6) | 52.1(2.5) | 99.31       | 4.88        | 100   |
---------------------------------------------------------------------------------------------

## References

  * [Application of rule induction algorithms for analysis of data collected by seismic  hazard monitoring systems in coal mines.](https://actamont.tuke.sk/pdf/2013/n4/7sikora.pdf)
  * [A Study of Rockburst Hazard Evaluation Method in Coal Mine](https://www.hindawi.com/journals/sv/2016/8740868/#B13)
  * [Classification: Basic concepts, decision trees and model evaluation](https://www-users.cs.umn.edu/~kumar001/dmbook/ch4.pdf)
  * [A gentle intro to LASSO regularisation using R](https://eight2late.wordpress.com/2017/07/11/a-gentle-introduction-to-logistic-regression-and-lasso-regularisation-using-r/)
  




